{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PV system data filtering test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfiltering functions for cleaning datasets. Specifically for pv applications.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "filtering functions for cleaning datasets. Specifically for pv applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import datatools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pvlib\n",
    "import pvanalytics\n",
    "import missingno as msno\n",
    "\n",
    "%matplotlib inline \n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "mpl.rcParams['font.size']=12\n",
    "mpl.rcParams['lines.linewidth']=1\n",
    "mpl.rcParams['xtick.labelsize']=10\n",
    "#mpl.rcParams['font.weight']='bold'\n",
    "mpl.rcParams['axes.titlesize']=22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use database values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a database using datatools\n",
    "def import_database(tablename, start, end, save=False):\n",
    "    engine = datatools.database.create_mss_engine()\n",
    "\n",
    "    sys_sql = f\"select * from {tablename} where TmStamp between '{start}' and '{end}';\"\n",
    "    #sys_sql = f\"select * from {tablename}\"\n",
    "    sys = pd.read_sql(sys_sql, engine)#, index_col='TmStamp')\n",
    "    sys.TmStamp = pd.to_datetime(sys.TmStamp)\n",
    "    sys = sys.set_index('TmStamp')\n",
    "    sys.index = sys.index.tz_localize('MST') # pvlib requires indexes to be timezone aware or it assumes UTC\n",
    "    print([sys.index.min(),  sys.index.max()], [sys.index.max()-sys.index.min()])\n",
    "    \n",
    "    if save==True:\n",
    "        sys.to_csv(f'{tablename}_{start}_{end}.csv')\n",
    "    \n",
    "    return sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use csv values\n",
    "\n",
    "before this there should be a way to get the database to save a csv file if you want to import. Or there should be a cleaned csv file that has filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd C:\\Users\\jtmcmul\\Desktop\\Jupyter_Notebook\\tools\\datatools-master\\System CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will read a csv file on your computer so you don't have to download data from the database\n",
    "def file_reader(name):\n",
    "    \"\"\"\n",
    "    Be sure to set your directory path to where the file is located on your computer.\n",
    "    \"\"\"\n",
    "    \n",
    "    sys = pd.read_csv(name)\n",
    "    sys.TmStamp = pd.to_datetime(sys.TmStamp)\n",
    "    sys['TS'] = sys.TmStamp\n",
    "    sys = sys.set_index('TmStamp')\n",
    "    [sys.index.min(),  sys.index.max()]\n",
    "    sys.name = name\n",
    "    \n",
    "    return sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple data\n",
    "only uses important columns during development phase. drops unnescessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this should drop stuff based on universal naming later on\n",
    "#Redo this so that it only keeps the names that you want. This makes it more usable for other data\n",
    "def drop_unnescessary_columns(df):\n",
    "    df = df.drop(['POACleanRC_E_Avg', 'GHI_Avg', 'Albedo_Avg', 'RecNum',\n",
    "                  'PH2_V_Avg','PH3_V_Avg','PH4_V_Avg',\n",
    "                  'PH2_I_Avg','PH3_I_Avg','PH4_I_Avg',\n",
    "                  'LM1_V_Avg','LM2_V_Avg','LM3_V_Avg','LM4_V_Avg',\n",
    "                  'LM1_I_Avg','LM2_I_Avg','LM3_I_Avg','LM4_I_Avg',\n",
    "                  'PH2_RTD_Avg','PH3_RTD_Avg','PH4_RTD_Avg',\n",
    "                  'LM1_RTD_Avg','LM2_RTD_Avg','LM3_RTD_Avg','LM4_RTD_Avg',\n",
    "                  'Wind_Gust_Max','Wind_Dir_std','Wind_Direction_Avg'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#universal naming, This will rename the column based on the dictionary you use.\n",
    "#this could be replaced with the rename function since it's pretty straightforward.\n",
    "def naming(df, dictionary):\n",
    "    df = df.rename(columns=dictionary)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metadata tracking\n",
    "\n",
    "this should be implemented into every filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_col(df):\n",
    "    #add a new metadata column\n",
    "    df['metadata'] = '0'\n",
    "    df['metadata'].astype(str)\n",
    "    \n",
    "    #add the convention as a dictionary\n",
    "    md = {'Code':[0],\n",
    "          'Description':['Raw data']}\n",
    "\n",
    "    #make a new dataframe to hold the names\n",
    "    df_meta = pd.DataFrame(data=md) \n",
    "    \n",
    "    return df, df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a - Shape and Interval Freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find recording interval and reporting period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these functions should be combined\n",
    "# find rows and columns\n",
    "\n",
    "def shape(sys):\n",
    "    \"\"\"\n",
    "    Tells you the number of rows and the number of columns of the dataframe. Uses a Pandas DataFrame as input.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sys : df\n",
    "        Pandas dataframe.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    rc\n",
    "        A string telling you the number of rows and columns\n",
    "    \"\"\"\n",
    "    nrows, ncols = sys.shape\n",
    "    rc = print(f'Number of rows = {nrows}, Number of columns = {ncols}')\n",
    "    return rc\n",
    "\n",
    "def interval_freq(sys, plot=True, apply_working_mask=True):\n",
    "    \"\"\"\n",
    "    Tells you the recording interval\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sys : df\n",
    "        Pandas dataframe.\n",
    "    \n",
    "    t : int\n",
    "        the resolution of your data. \n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    monitoring_availability\n",
    "        The fraction of the time that your system was on.\n",
    "        \n",
    "    recording_interval\n",
    "        the interval over which your data exists.\n",
    "        \n",
    "    freq\n",
    "        the frequency of your data.\n",
    "    \"\"\"\n",
    "    initial_no_of_rows = len(sys)\n",
    "    #finds the frequency of the data by looking at the interval between the first datasets\n",
    "    freq = pd.infer_freq(sys.index[:10])\n",
    "    #this will resample the data based on the infered frequency above. \n",
    "    #If there are multiple values in the frequency, then the resample will choose the median value.\n",
    "    sys = sys.resample(freq).median()\n",
    "    \n",
    "    no_of_rows = len(sys)\n",
    "    \n",
    "    #recording interval is the time between two consecutive intervals\n",
    "    d = {'T':'Minutes', 'S':'Seconds', 'M':'Months', 'Y':'Years'}\n",
    "    #replace the string so that it's easier to read\n",
    "    if any(char.isdigit() for char in freq)==False:\n",
    "        recording_interval = '1 ' + d[freq[-1]]\n",
    "    else:\n",
    "        recording_interval = freq[:-1] + ' ' + d[freq[-1]]\n",
    "    \n",
    "    #reporting period is the total amount of time being recorded\n",
    "    reporting_period = [sys.index[0], sys.index[-1]], sys.index[-1]-sys.index[0]\n",
    "    \n",
    "    #fraction of primary data that wasn't recorded. this accounts for nighttime and flashtest values\n",
    "    working_mask = (sys.Iave < 0.5) & (sys.irradiance >200) | (sys.irradiance<20) | (sys.Iave<0.1)\n",
    "\n",
    "    sys_ = sys.drop(sys[working_mask].index)\n",
    "    #add additional mask to sys_ to be filtered by night, do this by sun position\n",
    "    monitoring_availability_primary = len(sys_)/len(sys.Vave.asfreq('T'))\n",
    "    \n",
    "    #this is the fraction of data that is available because of missing time steps\n",
    "    monitoring_availability_raw = 100*(initial_no_of_rows/no_of_rows)\n",
    "    \n",
    "    \n",
    "    if plot==True:\n",
    "        #plot a scatter plot that shows where the data was taken out\n",
    "        plt.scatter(sys_.index, sys_.Iave, s=6, label='original values')\n",
    "        plt.scatter(sys.index[working_mask], sys.Iave[working_mask], s=6, label='dropped values')\n",
    "        plt.legend()\n",
    "        plt.title('Plot of Available Data')\n",
    "        plt.ylabel('Current')\n",
    "        plt.xlabel('Time')\n",
    "        plt.show()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    if apply_working_mask==True:\n",
    "        #use the working_mask to drop the data that isn't interesting to the problem.\n",
    "        working_mask = sys.drop(working_mask[working_mask==True].index)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return round(monitoring_availability_primary,4)*100, recording_interval, reporting_period, working_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other ways you can do the add missing time step is to use sys.reindex and sys.asfreq(freq)\n",
    "\n",
    "def add_missing_time_steps(df, df_meta, freq, save=False):\n",
    "    \n",
    "    df_comp  = df.asfreq(freq)\n",
    "    \n",
    "    \n",
    "    if save==True:\n",
    "        #Save csv\n",
    "        df_comp.to_csv(f'{df.index[0]}_to_{df.index[-1]}_{df.name}_COMPLETE.csv')\n",
    "        \n",
    "    #Updates the metadata column\n",
    "    df_comp.metadata = df_comp.metadata.replace(np.nan, '1')\n",
    "    \n",
    "    #adds the metadata code to the meta dataframe\n",
    "    md = {'Code':[1],\n",
    "          'Description':['Time steps added']}\n",
    "    \n",
    "    df_meta_1 = df_meta.append(pd.DataFrame(data=md), ignore_index=True, )    \n",
    "    \n",
    "    return df_comp, df_meta_1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_reader(x):\n",
    "    for i in x: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_masks(sys):\n",
    "    \n",
    "    #create new dataframe to hold masks\n",
    "    sys_comp_masks = pd.DataFrame(index=sys.index)#, columns = sys.columns[:-1])\n",
    "\n",
    "    #Create dataframe that describes the type of mask being applied.\n",
    "    md = {'Mask Name':[np.nan], 'Mask Description':[np.nan]}\n",
    "    sys_comp_masks_desc = pd.DataFrame(columns=md)\n",
    "\n",
    "    return sys_comp_masks, sys_comp_masks_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "incorporate metadata?\n",
    "\n",
    "# 3a - daylight filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daylight_filter_position(sys, sys_masks, sys_masks_desc, sys_meta, \n",
    "                             latitude, longitude, degree,\n",
    "                             apply_mask=True, plot=True):\n",
    "    #Calculate SunPositions using latitude and longitude\n",
    "    \n",
    "    \n",
    "    solpos = pvlib.solarposition.get_solarposition(sys.index, latitude, longitude)\n",
    "    dni_extra = pvlib.irradiance.get_extra_radiation(sys.index)\n",
    "    solpos = solpos.tz_convert('MST')\n",
    "\n",
    "    #come back to add syntax for .custom degree angle\n",
    "    #Apply daylight filter if elevation is greater than 10 degrees\n",
    "    mask_day = (solpos['apparent_elevation']<degree) | (sys.irradiance<0)\n",
    "    mask_day.name = 'mask_day'\n",
    "    print(f'Percent of data that is daylight = {round(mask_day.values.mean()*100,3)}%')\n",
    "    min_POA = min(sys.loc[~mask_day,'irradiance'].values)\n",
    "    print(f'Minimum POA irradiance during daylight = {round(min_POA,3)} W/m^2')\n",
    "\n",
    "    #Add to mask dataframe (df_masks)\n",
    "    sys_masks['mask_day_position']=mask_day\n",
    "\n",
    "    #Document mask description\n",
    "    md = {'Mask Name':['mask_day_position'], 'Mask Description':['Apparent elevation > 10']}\n",
    "    sys_masks_desc = sys_masks_desc.append(pd.DataFrame(data=md, index=[0]))\n",
    "\n",
    "\n",
    "    #update metadata\n",
    "    datelist = mask_day.loc[mask_day==False].index\n",
    "    sys.loc[datelist,'metadata'] = sys.loc[datelist,'metadata']+', 2.1'\n",
    "    md = {'Code':[2.1],\n",
    "          'Description':['Daylight filter: Sun Position']}\n",
    "\n",
    "    sys_meta_2 = sys_meta.append(pd.DataFrame(data=md), ignore_index=True )\n",
    "    \n",
    "    #optional plotting function\n",
    "    if plot==True:\n",
    "        sys_ = sys.drop(sys[mask_day].index)\n",
    "        #tell how much data is lost\n",
    "        print('sys:',len(sys))\n",
    "        print('sys with filter:',len(sys_))\n",
    "        print('remaining fraction of usable data:', round(len(sys_)/len(sys)*100,3),'%. Dropped points:', np.abs(len(sys_)-len(sys)))\n",
    "        #plot a scatter plot that shows where the data was taken out\n",
    "        plt.scatter(sys_.index, sys_.irradiance, color='blue')\n",
    "        plt.scatter(sys.index[mask_day], sys.irradiance[mask_day], color='orange')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Irradiance W/m2')\n",
    "        plt.show()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #this will apply the mask and drop the mask_day values that are True\n",
    "    if apply_mask==True:\n",
    "        sys_masks = sys_masks.drop(sys[mask_day].index)\n",
    "        sys = sys.drop(sys[mask_day].index)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    return sys, sys_masks, sys_masks_desc, sys_meta_2\n",
    "\n",
    "def daylight_filter_irradiance(sys, sys_masks, sys_masks_desc, sys_meta,\n",
    "                               apply_mask=True, plot=True):\n",
    "    #day time if irradiance is greater than 20 w/m2\n",
    "    mask_day = sys['irradiance']>20\n",
    "    mask_day.name = 'mask_day'\n",
    "    print(f'Percent of data that is daylight = {round(mask_day.values.mean()*100,3)}%')\n",
    "    min_POA = min(sys.loc[mask_day,'irradiance'].values)\n",
    "    print(f'Minimum POA irradiance during daylight = {round(min_POA,3)} W/m^2')\n",
    "\n",
    "    #Add to mask dataframe (df_masks)\n",
    "    sys_masks['mask_day_irradiance']=mask_day\n",
    "\n",
    "    #Document mask description\n",
    "    md = {'Mask Name':['mask_day_irradiance'], 'Mask Description':['Irradiance > 20']}\n",
    "    sys_masks_desc = sys_masks_desc.append(pd.DataFrame(data=md, index=[0]))\n",
    "    sys_masks_desc\n",
    "\n",
    "    #update metadata\n",
    "    datelist = mask_day.loc[mask_day==False].index\n",
    "    sys.loc[datelist,'metadata'] = sys.loc[datelist,'metadata']+', 2.2'\n",
    "    #metadata reference key update\n",
    "    md = {'Code':[2.2],\n",
    "          'Description':['Daylight filter: irradiance']}\n",
    "\n",
    "    sys_meta_2 = sys_meta.append(pd.DataFrame(data=md), ignore_index=True )\n",
    "    \n",
    "    #optional plotting function\n",
    "    if plot==True:\n",
    "        sys_ = sys.drop(sys[~mask_day].index)\n",
    "        #tell how much data is lost\n",
    "        print('sys:',len(sys))\n",
    "        print('sys with filter:',len(sys_))\n",
    "        print('remaining fraction of usable data:', round(len(sys_)/len(sys),3)*100,'%. Dropped points:', np.abs(len(sys_)-len(sys)))\n",
    "        #plot a scatter plot that shows where the data was taken out\n",
    "        plt.scatter(sys_.index, sys_.irradiance)\n",
    "        plt.scatter(sys.index[~mask_day], sys.irradiance[~mask_day])\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Irradiance W/m2')\n",
    "        plt.show()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #this will apply the mask and drop the mask_day values that are True\n",
    "    if apply_mask==True:\n",
    "        sys_masks = sys_masks.drop(sys[~mask_day].index)\n",
    "        sys = sys.drop(sys[~mask_day].index)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    return sys, sys_masks, sys_masks_desc, sys_meta_2\n",
    "\n",
    "def daylihgt_filter_time():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a physical filters\n",
    "\n",
    "outliers are replaced with na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to add mask and description to dataframes\n",
    "# Is there a way to not have to pass the dataframe into the function but rather define a basename and have \n",
    "#   the function figure out the right df to work with?  This would avoid errors.\n",
    "\n",
    "def add_mask_desc(df_masks_desc, mask, description):\n",
    "    entry = {'Mask Name':[mask.name], 'Mask Description':[description]}\n",
    "    lastindex = max(df_masks_desc.index, default=0)\n",
    "    df2 = pd.DataFrame(data=entry, index=[lastindex+1])\n",
    "    df_new = df_masks_desc.append(df2)\n",
    "    return df_new\n",
    "\n",
    "def add_mask_df(df_masks,mask):\n",
    "    df_masks[mask.name] = mask.values\n",
    "    return df_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#physical limits, give it a column, start, end, and unit\n",
    "\n",
    "def physical_filter(sys, variable, lower, upper, sys_comp_masks, sys_comp_masks_desc, sys_meta):\n",
    "    \n",
    "    variable = sys[variable]\n",
    "    # Physical Limits  (note: mask = True for data in acceptable ranges)\n",
    "    mask_low = variable >= lower\n",
    "    mask_high = variable <= upper\n",
    "    #mask_nan = ~varible.isnull() #True = data is not NaN  --> Maybe do this check later\n",
    "    mask = mask_low | mask_high  ### Should add a check if the values are NaNs?  \n",
    "    mask.name = f'mask_{variable.name}'\n",
    "\n",
    "    #Add to documentation trail\n",
    "    sys_comp_masks = add_mask_df(sys_comp_masks,mask)\n",
    "    #how to drop a column that is already there? if for name in columns == variable.name: ????\n",
    "    #sys_comp_masks.drop([sys1.poa.name],axis=1)\n",
    "    sys_comp_masks_desc = add_mask_desc(sys_comp_masks_desc,mask, f'{variable.name} >={lower}. OR {variable.name} <= {upper}.')\n",
    "    \n",
    "    \n",
    "    #metadata\n",
    "    datelist = mask.loc[mask==False].index\n",
    "    code = round(sys_meta.iloc[-1,0]) + 1\n",
    "    sys.loc[datelist,'metadata'] = sys.loc[datelist,'metadata']+f', {code}'\n",
    "    md = {'Code':[code],\n",
    "          'Description':[f'Physical filter: {variable.name}']}\n",
    "\n",
    "    sys_meta_2 = sys_meta.append(pd.DataFrame(data=md), ignore_index=True )\n",
    "    \n",
    " \n",
    "    \n",
    "    return sys, sys_comp_masks, sys_comp_masks_desc, sys_meta_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison with other sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum change between successive data points. Give it a column, limit, and unit\n",
    "\n",
    "def diff_filter(sys, variable,maxchange, sys_comp_masks, sys_comp_masks_desc, sys_meta ):\n",
    "    \n",
    "    variable = sys[variable]\n",
    "\n",
    "    #mask used for the step change in ambient temperature Tamb\n",
    "\n",
    "    sys[f'{variable.name}_diff'] = abs(variable.diff())\n",
    "\n",
    "    diff_mask = sys[f'{variable.name}_diff']<=maxchange\n",
    "\n",
    "    #Add to documentation trail\n",
    "    sys_comp_masks = add_mask_df(sys_comp_masks,diff_mask)\n",
    "    sys_comp_masks_desc = add_mask_desc(sys_comp_masks_desc,diff_mask, f'change in {variable.name} <= {maxchange}')\n",
    "    \n",
    "    #metadata\n",
    "    datelist = diff_mask.loc[diff_mask==False].index\n",
    "    code = round(sys_meta.iloc[-1,0]) + 1\n",
    "    sys.loc[datelist,'metadata'] = sys.loc[datelist,'metadata']+f', {code}'\n",
    "    md = {'Code':[code],\n",
    "          'Description':[f'Step change filter: {variable.name}']}\n",
    "\n",
    "    sys_meta_2 = sys_meta.append(pd.DataFrame(data=md), ignore_index=True )\n",
    "   \n",
    "    return sys, sys_comp_masks, sys_comp_masks_desc, sys_meta_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical and comparative tests. Give it a column, standard deviations, \n",
    "\n",
    "def stat_sigma_test(data, deviation=3, apply=False):\n",
    "        \n",
    "    mean = np.mean(data)\n",
    "    stdv = np.std(data)\n",
    "\n",
    "\n",
    "    z = np.abs((data-mean)/stdv)\n",
    "\n",
    "    mask =  z > deviation #true is real values, false is bad values\n",
    "\n",
    "    data_nan = data.mask(mask,np.nan)\n",
    "    \n",
    "    if apply==True: #this will apply the mask to the data if true\n",
    "        return data_nan, mask\n",
    "    else:\n",
    "    #if apply==False: #this will only return the mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hampel_filter(input_series, window_size, n_sigmas=3):\n",
    "\n",
    "    k = 1.4826 # scale factor for Gaussian distribution\n",
    "    new_series = input_series.copy()\n",
    "\n",
    "    # helper lambda function \n",
    "    MAD = lambda x: np.median(np.abs(x - np.median(x)))\n",
    "    \n",
    "    rolling_median = input_series.rolling(window=2*window_size, center=True).median()\n",
    "    rolling_mad = k * input_series.rolling(window=2*window_size, center=True).apply(MAD)\n",
    "    diff = np.abs(input_series - rolling_median)\n",
    "\n",
    "    indices = np.argwhere(np.array(diff > (n_sigmas * rolling_mad))).flatten()\n",
    "    new_series[indices] = rolling_median[indices]\n",
    "    \n",
    "    return new_series, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b\n",
    "\n",
    "search for na, nan, and black cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter(sys, masks, *args):\n",
    "    \n",
    "    sys_nan = sys.where(masks.mask_day, np.nan)\n",
    "    \n",
    "    #add documentation to word file\n",
    "    #document = Document()\n",
    "    #document.add_heading('Document Title', 0)\n",
    "    \n",
    "    for var in args:\n",
    "        sys_nan[f'{var}'] = sys_nan[f'{var}'].where(masks[f'mask_{var}'], np.nan)\n",
    "\n",
    "        #p = document.add_paragraph('A plain paragraph having some ')\n",
    "    \n",
    "     \n",
    "    \n",
    "    return sys_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5a\n",
    "\n",
    "identify missing data mechanism (MCAR, MAR, NMAR) by using visualization method.\n",
    "\n",
    "Use the msno library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_visual(df):\n",
    "    msno.matrix(df,freq='T')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a function that applies the mask to the data so it shows which points are being masked in red.\n",
    "def visual_dots(sys, mask):\n",
    "\n",
    "    masked_sys = sys[~mask]\n",
    "    plt.scatter(sys.index, sys, c='#add8e6',s=6, label='data', alpha=0.4)\n",
    "    plt.scatter(masked_sys.index, masked_sys, c='#da9b86', label='mask')\n",
    "    plt.xlabel(sys.index.name)\n",
    "    plt.ylabel(sys.name)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual inspection of scatter plots\n",
    "def scatter_inspect(df):\n",
    "    \n",
    "    for i in df.columns:\n",
    "        if i == 'metadata':\n",
    "            continue\n",
    "        y = df[f'{i}']\n",
    "        x = df.index\n",
    "        plt.scatter(x,y, s=4)\n",
    "        plt.title(f'{i}')\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing data rates\n",
    "\n",
    "Identify missing data rate and missingness rate for every recorded field measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_rates(sys, variables):\n",
    "    \n",
    "    total_length = len(sys.index)\n",
    "    \n",
    "    for var in variables:\n",
    "        print(f'{var} missing rate:', sys[f'{var}'].isna().sum()/total_length*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6a\n",
    "\n",
    "if the missing data rate is less than 10%\n",
    "- Discard the missing period (listwise deletion) or\n",
    "- Infer the missing measurements for (a) a whole month missing for a yearly performance analysis and (b) providing robust degradation and performance loss rate estimates\n",
    "\n",
    "\n",
    "if the missing data rate is greater than 10%\\\n",
    "- if meteorlogical data is available then infer the missing data using an empirical model\n",
    " - To fill in missing power measurements use SAPM model from PVLIB\n",
    " - To fill in missing module temperatures use the SMTM (or the Ross thermal model35 when only GI and Tamb are available)\n",
    " \n",
    " \n",
    "- if meteorlogical data is unavailable then use univariate data imputation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less than 10% of the data is missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    " def simple_filter(sys, variable, lower=None, upper=None, apply=False):\n",
    "\n",
    "    variable = sys[variable]\n",
    "\n",
    "# Physical Limits  (note: mask = True for data in acceptable ranges)\n",
    "\n",
    "\n",
    "    if lower==None:\n",
    "        mask_low = pd.Series(dtype='float64')\n",
    "        if upper==None:\n",
    "            return print('Please specify upper and/or lower limits')\n",
    "        else:\n",
    "            mask_high = variable<=upper\n",
    "            mask = mask_high\n",
    "    else:\n",
    "        mask_low = variable >= lower\n",
    "        if upper==None:\n",
    "            mask_high = pd.Series(dtype='float64')\n",
    "            mask = mask_low\n",
    "        else:\n",
    "            mask_high = variable<= upper\n",
    "            mask = mask_low & mask_high\n",
    "        \n",
    "### Should add a check if the values are NaNs?  \n",
    "    mask.name = f'mask_{variable.name}'\n",
    "\n",
    "    if apply==True:\n",
    "        sys = sys.where(mask, np.nan)\n",
    "        return sys\n",
    "    else:\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PVModel",
   "language": "python",
   "name": "pvmodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
